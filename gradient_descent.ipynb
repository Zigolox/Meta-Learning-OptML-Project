{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import equinox as eq\n",
    "from jax.config import config\n",
    "#config.update(\"jax_debug_nans\", True)\n",
    "#config.update(\"jax_disable_jit\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_function(x):\n",
    "    return x.T@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(14, dtype=int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadratic_function(jnp.array((1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT makes everything faster\n",
    "@eq.filter_jit\n",
    "def gradient_descent(lr,t,function,x,kwargs=None):\n",
    "    \n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    # Take the gradient of the function\n",
    "    par_fun = partial(function,**kwargs)\n",
    "    grad_function = jax.value_and_grad(par_fun)\n",
    "    # We create a step function which matches the API of lax.scan\n",
    "    \n",
    "    def step(x,_):\n",
    "        v, g = grad_function(x)\n",
    "        x -= lr*g\n",
    "        return x, x\n",
    "    \n",
    "    # lax.scan is like a very fast for-loop we can jit and take a gradient through\n",
    "    x, xs = jax.lax.scan(step,x,None,t)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 2.0370368e-10, -4.0740736e-10,  2.0370368e-10], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(0.1,100,quadratic_function,jnp.array((1.,-2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(lr,**kwargs):\n",
    "    x = gradient_descent(lr=lr,**kwargs)\n",
    "    return x.T@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37897322\n"
     ]
    }
   ],
   "source": [
    "# We now want to do meta learning\n",
    "# This means we optimize the gradient descent parameter lr using gradient descent\n",
    "# For this we need to make the normal gradient descent function only take in one argument\n",
    "args = {\"t\": 3, \"function\": quadratic_function, \"x\": jnp.array((1.,-2,1))}\n",
    "lr = gradient_descent(0.0005,1000,loss,0.1,args)\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
